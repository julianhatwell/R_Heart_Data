---
title: "heart"
author: "Julian Hatwell"
date: '`r format(Sys.time(), "%b %Y")`'
output: html_document
---

```{r prologue, include=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE
              , message = FALSE
              , echo = FALSE
              )
opts_template$set(
  fig.wide = list(fig.height = 4.5, fig.width = 8, fig.align='center')
  , fig.wideX = list(fig.height = 3, fig.width = 9, fig.align='center')
  , fig.relaxed = list(fig.height = 6, fig.width = 8, fig.align='center')
  , fig.hugetile = list(fig.height = 7, fig.width = 7, fig.align='center')
  , fig.bigtile = list(fig.height = 5, fig.width = 5, fig.align='center')
  , fig.tile = list(fig.height = 3, fig.width = 3, fig.align='center')
)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

par(mar = c(4,3,3,1))

set.seed(142136)
```

```{r initial_load}
library(readr)
library(dplyr)
library(lattice)
library(ggplot2)
library(corrplot)
library(vcd)
library(psych)
library(car)
library(VIM)
library(ca)
library(factoextra)
library(cluster)
library(clustertend)
library(caret)
source("HeartTheme.R")

# load data
heart <- read.csv("Heart.csv")
```

# Analysis of the UCI Heart Data

## Introduction
We will conduct an analysis of the UCI Machine Learning Repository [heart data set](https://archive.ics.uci.edu/ml/datasets/heart+disease). This data set is often used for demonstrating classification methods. The target variable is usually AtheroscleroticHDisease, which indicates the presence or absence of pathologies of the blood vessels that supply the heart muscle itself. We will do something slightly different here and demonstrate several unsupervised methods to detect associations and clusters using just the independent variables. We will show the link between clustering and dimension reduction by demonstrating that cluster membership is an accurate indicator of heart disease.

### Research Questions

1. Are there patterns or clusters among the independent variables?
1. If so, is cluster membership associated with the presence of heart disease?

### Analytic Strategy
We will proceed in two phases:

1. An initial exploratory analysis
    1. to assess the data quality and perform any necessary cleansing.
    1. develop an intiution of the distributions and interactions between variables. This will comprise of visual analytics with density plots, box plots, fourfold plots and mosaic plots.
1. We will compare various unsupervised learning methods:
    1. Various matrix decomposition methods, including PCA, Correspondence Analysis and Exploratory Factor Analysis
    1. Clustering methods, including hierarchical and distance based methods.

<!-- We will conclude by developing heuristics (rules of thumb) based on the earlier results to aid with medical diagnostics -->

## Exploratory Analysis
The data dictionary provides the following information:

```{r variables}
variables_df <- data.frame(name = c("AtheroscleroticHDisease"
                                    , "Age", "Sex", "ChestPain"
                                    , "RestBP", "Chol", "Fbs"
                                    , "RestECG", "MaxHR"
                                    , "ExAng", "Oldpeak"
                                    , "Slope", "Ca", "Thal")
                           , type = c("factor", "integer"
                                      , "factor", "factor"
                                      , "integer", "integer"
                                      , "factor", "factor"
                                      , "integer", "factor"
                                      , "factor", "factor"
                                      , "small integer", "factor")
                           , notes = c("Presence of heart disease"
                                       , "Age in years", ""
                                       , "Presence/type of chest pain"
                                       , "Resting blood pressure mm Hg"
                                       , "Serum cholesterol mg/dl"
                                       , "Fasting blood sugar"
                                       , "Resting electrocardiograph results"
                                       , "Maximum heart rate during exercise"
                                       , "Exercise induced angina"
                                       , "ST depression exercise relative to rest"
                                       , "Slope of peak ST exercise segment"
                                       , "Number of major vessels under fluoroscopy"
                                       , "No description given"
                                       ))
kable(variables_df)
```

There are `r nrow(heart)` rows and `r ncol(heart)` columns.

```{r eda_pass1}
summary(heart)
```

### First Pass
X is an index number column and should be removed. AtheroscleroticHDisease is too much to type, so we will change the name. Several variables are coded as numeric but are factors; Sex, for example. The documentation [here](https://archive.ics.uci.edu/ml/datasets/heart+disease) is informative. These need to be recoded.

```{r recoding, echo = TRUE}
# recoding
heart <- heart %>% select(-X) %>%
  rename(HDisease = AtheroscleroticHDisease) %>%
  mutate(Sex = factor(ifelse(Sex == 1, "M", "F"))
         , Fbs = factor(ifelse(Fbs == 1, ">120", "<=120"))
         , RestECG = factor(ifelse(RestECG == 0, "normal"
                                   , "abnormal")) # there are only 4 valued at 1, we'll reduce to just two levels
         , ExAng = factor(ifelse(ExAng == 1, "Yes", "No"))
         , Slope = factor(ifelse(Slope == 1, "down", "level")) # there few valued at 3, we'll reduce to just two levels
         , Ca = factor(Ca, ordered = TRUE))
```

### General Housekeeping
There is a very small number of missing values, which we would like to impute based on row-wise information. The figure demonstrates that there are no instances that share missingness in both the variables involved. The non-parametric nearest neighbours imputation is a reasonable choice, as it makes no assumptions about the data.

```{r missingness, opts.label='fig.wideX'}
image(is.na(heart[, c("Ca", "Thal")])
      , main = "Missing Values"
      , xlab = ""
      , ylab = "Ca, Thal"
      , xaxt = "n"
      , yaxt = "n"
      , bty = "n"
      , col = c(myPal[4]
                   , myPalDark[5])
                   )
```

A kernel density plot of each numeric variables reveals skew and non-normality in the Oldpeak variable. Power transformations are unsuitable because of the prevalence of zero values, so this will be left as is. There is an outlier in the Chol variable. Briefly researching this matter online, it is apparent that a reading of >200 for cholesterol is already considered extremely high and the problematic reading in our dataset is nearly 600.

```{r density_nums, opts.label='fig.wideX'}
classes <- sapply(heart, function(x) class(x)[1])
num_vars <- names(classes)[classes %in% c("integer", "numeric")]
cat_vars <- names(classes)[!(classes %in% c("integer", "numeric"))]

for (var in num_vars) {
d <- densityplot(~heart[[var]]
                 , xlab = var
                 , par.settings = myLatticeTheme)
print(d)
}

cat("Skew")
sapply(num_vars, function(x) skew(heart[[x]]))
```

Values of skew >1 are severe. The reading for Chol is a result of the outlier. Oldpeak is evidently not normally distributed, and the range is not very large. There is no reason to transform this variable.

Our first pass at exploring the data suggests the following actions:

1. Set the outlying Chol value to NA,
1. Impute missing values for Chol, Ca and Thal, and
1. Scale all the variables between [0,1], to support our clustering techniques.

```{r minna}
minval <- function(x) min(x, na.rm = TRUE)
maxval <- function(x) max(x, na.rm = TRUE)
```


```{r fixes, echo = TRUE}
# change outlier to missing
heart$Chol[which.max(heart$Chol)] <- NA

# scale to [0, 1], my functions exclude the NA
for (nv in num_vars) {
  if(minval(heart[[nv]]) < 0) {
    heart[[nv]] <- (heart[[nv]] + abs(minval(heart[[nv]]))) / 
      (maxval(heart[[nv]]) + abs(minval(heart[[nv]])))  
  } else {
    heart[[nv]] <- (heart[[nv]] - minval(heart[[nv]])) / 
      (maxval(heart[[nv]]) - minval(heart[[nv]]))
  }
}

# impute missing - VIM package. Median is the default function.
heart <- kNN(heart, c("Chol", "Ca", "Thal"), imp_var = FALSE, k=11) 
```

```{r tidy2, opts.label='fig.wideX'}
cat("Recoded data set")
summary(heart)
```

### Bivariate Distributions
We would now like to determine which variables might be good predictors of atherosclerotic heart disease and which features may be interacting. A simple bivariate correlation analysis is followed by a scatterplot matrix conditioned on each level of HDisease.

```{r corrplotting, opts.label='fig.bigtile'}
corrplot(cor(heart[num_vars]), order="AOE", type="upper"
        , col = myPal.rangeDiv(10)
        , tl.pos="d", tl.cex = 1, tl.col = myPalDark[1]
        , method = "number", number.cex = 1.5)
corrplot(cor(heart[num_vars]), add=TRUE, type = "lower"
        , col = myPal.rangeDiv(10)
        , method = "ellipse", order = "AOE", diag = FALSE
        , tl.pos="n", cl.pos = "n")
```

```{r splomming, opts.label='fig.relaxed'}
trel <- myLatticeTheme
trel$plot.symbol$col <- myPal[1]
splom(~heart[num_vars] | HDisease
      , data = heart
      , diag.panel = function(x, ...){
        yrng <- current.panel.limits()$ylim
        d <- density(x, na.rm=TRUE)
        d$y <- with(d, yrng[1] + 0.95 * diff(yrng) * y / max(y) )
        panel.lines(d)
        diag.panel.splom(x, ...)
 }
      , panel = function(x, y, ...){
        panel.xyplot(x, y, ..., alpha = 0.4)
        panel.loess(x, y, ..., col = myPalDark[2], lwd = 3)
      }
      , main = "Scatterplot Matrix by HDisease Group"
      , xlab = ""
      , layout = c(2, 1)
      , pscales = 0
      , par.settings = trel)
```

Careful inspection of the splom shows a clear elevation of Oldpeak in the HDisease = Yes group. There is a potentially interesting interaction between Age and MaxHR: Specifically, MaxHR is more correlated with Age in the HDisease = No group, whereas MaxHR is slightly lower whatever the Age in the HDisease = Yes group. The HDisease = Yes group are formed from a narrower Age band, however. The remaining continuous variables may be less informative.

We now generate a boxplot for each of the five continuous variables, conditioned on each of the factor variables.

```{r boxplots, opts.label='fig.wideX'}
cols <- myPal.rangeDiv(5)
names(cols) <- num_vars
  
myBoxPlots <- function(nv, cv) {
  fmla <- as.formula(paste(nv, " ~ ", cv))
  boxplot(fmla, data = heart, col = cols[nv])
}
par(mfrow=c(1,5))
for (cv in cat_vars){
  for (nv in num_vars) {
    myBoxPlots(nv, cv)
  }
}
par(mfrow=c(1,1))
```

A visual instpection of the boxplots indicates that there may be some interaction between Chestpain and the three informative variables identified previously (Age, MaxHR and Oldpeak). This is also true of ExAng (exercise induced angina), Slope, Ca and Thal. We will not perform any statistical tests at this stage. It is sufficient to develop this intuition and fishing for p-values with so many tests requires careful attention to the false discovery rate, and assumptions checking and diagnostics for an unwieldy number of variable combinations.

### Multivariate Counts
Distributions among categorical variables can be assessed according to counts and proportions. The following Fourfold and Mosaic visualisations implicitly perform significance tests by means of shading residuals.

```{r mosaic1, opts.label='fig.tile'}
fourfold(with(heart, table(HDisease, Sex))
         , color = myFourFold)
```

```{r mosaic2, opts.label='fig.relaxed'}
fourfold(with(heart, table(HDisease, ExAng, ChestPain))
         , color = myFourFold)
```

We can see that the presence of atherosclerotic heart disease is significantly associated with males, while a third order interaction exists such that those who are generally free from chest pain but suffer with exercise induced angina have a significant risk of atherosclerotic heart disease.

```{r mosaic3, opts.label='fig.relaxed'}
mosaic(with(heart, table(HDisease, Slope, Ca, Thal))
       , gp = shading_hsv
       , gp_args = list(h = myShading["h", ]
                        , s = c(0.75, 0)
                        , v = 1
                        , lty = 1:2
                        , line_col = c(myPalDark[5], myPalDark[4])
                        )
       )
```

In this four way analysis, we can see that downward slope is most associated with the HDisease = No group and level slope for HDisease = Yes. Similarly, one or more arteries visible under fluoroscopy (Ca) are most strongly associated with HDisease = Yes. A third order interaction indicates that a combination of level Slope and reversible Thal is very strongly associated with HDisease = Yes, with fixed Thal also being strongly indicative.

## Unuspervised Learning Methods
We continue with a more advanced exploration of this data set using various clustering techniques

### PCA
The first method we will exlore is principle components analysis (PCA) on the scaled continuous variables. PCA identifies orthogonal projections of multivariate data that capture most of the variation in the first components.

```{r pca1, opts.label='fig.wideX'}
heart_pca <- prcomp(heart[num_vars], scale. = FALSE)
heart_pca
summary(heart_pca)
plot(heart_pca, main = "scree plot for heart pca")
```

We can see from the output summary that Age and Oldpeak are loaded onto PC1 in the opposite direction to MaxHR, indicating a negative correlation. The other PC's can generally be interpreted according to further relationships among these variables, some of which we have already seen in the bivariate correlation analysis. It seems from the cumulative variance measure and the scree plot that the projection of these five numeric features onto less interpretable principal components does not offer any obvious gains, because the it still takes four components to capture most of the variance. Nevertheless, we can make a biplot to better understand the multivariate relationships.

```{r pca2, opts.label='fig.bigtile'}
biplot(heart_pca
       , xlim = c(-0.2, 0.2), ylim = c(-0.2, 0.2)
       , col = c(myPal[2], myPalDark[5]), cex=c(0.5, 1.5))
```

### Multiple Correspondence Analysis
Multiple correspondence analysis is a dimension reduction and clustering analysis for categorical counts. The relationships among several or many categorical variables can be mapped in two dimensions, giving fairly intiuitive results. The most important benefit is that categorical variables are implicitly recoded into non-abritrary numeric values. This makes them available to use in any methods that only accept real-valued inputs, such as distance-based methods like k-means or hierarchical clustering.

```{r mca1, output.lines=10, opts.label='fig.relaxed'}
heart_mca <- mjca(heart[cat_vars])
summary(heart_mca)
plot(heart_mca, map = "symbiplot"
     , arrows = c(FALSE, TRUE)
     , mass = c(FALSE, TRUE)
     , contrib = c("none", "relative")
     , col = c(myPalDark[5], myPalDark[4]))
```

The results of this analysis are indeed rather interesting and confirm the findings of the initial explaratory analysis. The plot can be interpreted by identifying attributes that have moved in the same direction from the origin (0, 0), with particular interest in those that have clustered close together. We can see the specific values of ChestPain, Slope, Ca, ExAng and Sex that are associated with the presence of absence of heart diseease. We also have confirmation that Fbs is much less correlated with HDisease and RestECG only moderately correlated. These latter two variables might well be left out of any predictive model. We can re-run this analysis excluding those variables.

```{r mca2, output.lines=11, opts.label='fig.relaxed'}
heart_mca <- mjca(heart[cat_vars[!cat_vars %in% c("Fbs", "RestECG")]])
summary(heart_mca)
plot(heart_mca, map = "symbiplot"
     , mass = c(FALSE, TRUE)
     , contrib = c("none", "relative")
     , col = c(myPalDark[5], myPalDark[4]))
```

We are not displaying the arrows here because it is hard enough to read the crowded labels. The the clusters are very pronounced and we can identify a handful of less informative attributes. For example, Thal=fixed and ChestPain=typical represent small minorities in the way these variables are distributed.

We can augment this analysis by discretizing the numeric variables and including them. This will be done by binning into low and high levels.

```{r qbins, echo=TRUE}
twobins <- function(x, ind) {
  cut(x, breaks=c(quantile(heart[[ind]], probs = c(0, 0.5, 1))),
      labels = c("lo", "hi")
      )
}
heart_bins <- cbind(heart[cat_vars[!cat_vars %in% c("Fbs", "RestECG")]] # exclude based on latest findings
                    , sapply(num_vars
                            , function(ind) sapply(heart[[ind]]
                                                   , twobins
                                                   , ind = ind)))
```

```{r mca3, output.lines=14, opts.label='fig.relaxed'}
heart_mca <- mjca(heart_bins)
summary(heart_mca)
plot(heart_mca, map = "symmetric"
     , mass = c(FALSE, TRUE)
     , contrib = c("none", "relative")
     , col = c(myPalDark[5], myPalDark[4]))
```

This plot takes some time to interpret, but a careful look further confirms results of our previous analyses. The lowest MaxHR values are associated with heart disease and the highest values with absence of disease. Increasing Age is generally associated with presence of heart disease. Increasing Chol is associated, but only very moderately - the different attribute levels are not separated widely on the horizontal dimension. The same is true for RestBP. Let us re-run this once more, excluding these less correlated variables. We also exclude the HDisease label as we do not want it to influence the resulting values.

```{r mca4, output.lines=13, opts.label='fig.relaxed'}
heart_mca <- mjca(heart_bins[names(heart_bins)[!names(heart_bins) %in% c("Chol", "RestBP", "HDisease")]])
summary(heart_mca)
(res <- plot(heart_mca, map = "symmetric"
     , mass = c(FALSE, TRUE)
     , contrib = c("none", "relative")
     , col = c(myPalDark[5], myPalDark[4])))
```

We can finish with this multiple correspondence analysis that provides two very well differentiated clusters of attributes. This result gives us plenty of reason to believe that we could infer a diagnostic model from the data set. Because nearly all the inertia (variance) is captured on the first dimension, we can use the one-dimensional coordinate values as numerical proxies for the categorical attributes in the analyses to follow.

```{r mca_coords, echo=TRUE}
# res is the plot object created by (res <- plot(...))
coords <- data.frame(res$cols, heart_mca$factors)[-(20:25), ]
# the last 6 rows are the three continuous vars, and we just want to convert categorical to real valued.
# We don't need the HDisease feature as we will see how well the clusters match these labels.
heart_real <- heart[, num_vars[!num_vars %in% c("Chol", "RestBP")]]
for(c in cat_vars[!cat_vars %in% c("Fbs", "RestECG", "HDisease")]) {
  heart_real[[c]] <- rep(NA, nrow(heart))
}
for(i in 1:nrow(coords)) {
  fac <- as.character(coords$factor[i])
  lev <- as.character(coords$level[i])
  heart_real[[fac]] <- ifelse(heart[[fac]] == lev, coords$Dim1[i], heart_real[[fac]])
}
```

## Distance-based Clustering
Now that we have selected our variables of interest and converted categorical values to non-arbitrary real-values, we can continue with distance based clustering. The following visualisation is a map of all the Euclidean distances between the points.

```{r distances, opts.label='fig.hugetile'}
distance <- get_dist(heart_real)
myPalDiv <- myPal.rangeDiv(3)
fviz_dist(distance, show_labels = FALSE
          , gradient = list(low = myPalDiv[1], mid = myPalDiv[2], high = myPalDiv[3]))
```

This visualisation provides a useful confirmation of the clustering tendency in this data. We can also compute the [Hopkins statistic](https://en.wikipedia.org/wiki/Hopkins_statistic) *H*, which tests the null hypothesis that the data has come from a uniform distribution and is distributed as $H \sim \mathit{Beta}(n, n)$ where *n* is the number of samples used to calculate the statistic. A value of *n* = 5-10% of the dataset is [recommended](https://arxiv.org/pdf/1808.08317.pdf).

```{r hopstat, echo=TRUE}
n <- nrow(heart_real)/10
H <- hopkins(heart_real, n = n, header = TRUE)$H
H < qbeta(0.05, n, n) # significant result if TRUE
```

### K-medoids Clustering
K-medoids searches for k archetypal or representative instances, called medoids that act as the cluster centres. Each non-medoid instance is assigned to its nearest medoid. The algorithm proceeds by swapping medoid and non-medoid points, accepting a swap that decreases the sum of a dissimilarity function. K-medoids is less sensitive to outliers than the classic K-means method, so is often favoured. Another reason to prefer K-medoids is that the number k can be estimated using the silhouette method. From the visual inspection and the knowledge we have already that these variables tend to be associated with heart disease, we could assume two clusters is the correct number, but it's still worth checking.

Here we run the silhouette method to determine the best number for k:

```{r pamsils, opts.label='fig.wideX'}
fviz_nbclust(heart_real, pam, method = "silhouette"
             , k.max = 5, linecolor = myPalDark[4]) +
  myGgTheme()
```

Then we run the clustering with k=2:

```{r pam2}
pam2_clus <- pam(heart_real, k = 2, diss = FALSE, metric = "euclidean")
pam2_clus$medoids # translate back
table(pam2_clus$cluster, heart$HDisease)
```

Recall that HDisease was not included in the creation of the real-valued data or the clustering process. Cross-tabulating the instances from the original dataset shows a "confusion matrix-like" result, indicating that the two clusters have captured the association with heart disease among the variables of interest. The cluster labels are arbitrary in this unsupervised learning process. We can set the appropriate label to the cluster id and get a full suite of diagnostics:

```{r pam_diag, opts.label='fig.relaxed'}
cm <- confusionMatrix(factor(ifelse(pam2_clus$cluster == 1, "Yes", "No")), heart$HDisease)
cm
fviz_cluster(pam2_clus, heart_real
             , ellipse.type = "convex"
             , geom = "point"
             , palette = c(myPalDark[4], myPalDark[5])) +
  myGgTheme()
```

We can see that if we assigned unlabelled points to their nearest cluster medoid, we might expect an accuracy of around `r round(cm$overall["Accuracy"], 2)`
